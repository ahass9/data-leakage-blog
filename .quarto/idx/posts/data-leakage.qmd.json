{"title":"Why Your Model Performs Too Well: Understanding Data Leakage","markdown":{"yaml":{"title":"Why Your Model Performs Too Well: Understanding Data Leakage","author":"Ashifa Hassam","date":"2026-01-16","categories":["machine learning","data science"],"bibliography":"references.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nHave you ever trained a machine learning model that performed perfectly in testing, then failed in the real world? Perhaps your model scored 98% accuracy during testing, but when deployed on new data, performance dropped drastically. This frustrating scenario often happens because of a subtle, yet serious mistake: **data leakage**.\n\nData leakage is one of the most common pitfalls in data science. It can make models appear more powerful than they actually are, leading to false confidence, wasted resources, or even costly mistakes when deployed. In this post, I will explain what data leakage is, explore the types and causes, provide examples, and show how to prevent it. By the end, you will have a clear understanding of how to keep your models reliable and trustworthy.\n\n---\n\n## What is Data Leakage?\n\nIn machine learning, **data leakage occurs when your model has access to information during training that it would not realistically have when making predictions in the real world**. In other words, your model \"cheats\" by learning patterns that are impossible to know ahead of time. This may sound simple, but it can happen in subtle ways, such as including future information in your features or accidentally mixing training and testing data. The result is an inflated performance metric during development, which can be disastrous when the model is deployed.\n\n**Key takeaway:** Data leakage makes your model look great on paper, but it does not generalize to real world data.\n\n---\n\n## Common Types of Data Leakage\n\nThese are some of the main ways data leakage can happen:\n\n### 1. Train-Test Contamination\n\nThis occurs when the same data ends up in both the training and testing sets. For instance, imagine shuffling a dataset incorrectly or scaling features before splitting. Your model \"sees\" test data during training, leading to artifically high evaluation scores. [@ibm:data-leakage]\n\nExample: Fitting transformations on the entire dataset, including test data. \n\n### 2. Target Leakage\n\nTarget leakage occurs when a feature includes information that would only be available after the target outcome happens. This type of leakage is especially common in datasets with timestamps or events. [@ibm:data-leakage]\n\nExample: Predicting hospital readmissions using a feature \"Number of Medications After Discharge\" would be target leakage because this information is only known after the patient leaves the hospital. \n\n### 3. Temporal/Time Based Leakage\n\nTime-series and sequential data are prone to leakage if future information is used to predict the past. [@wiki:leakage_ml]\n\nExample: Predicting stock prices using feeatures that include future market movements. Even if it seems like a valid feature, the model is essentially \"peaking into the future.\"\n\n### 4. Preprocessing Leakage\n\nSome preprocessing steps can inadvertently leak information if applied before the train-test split. [@sailpoint:data-leakage]\n\nExample: Feature selection using the whole dataset.\n\n---\n\n## Detecting Data Leakage\n\nHow do you know if your model is suffering from data leakage? Below are some indicators:\n\n1. **Unrealistically high performance**: 99% accuracy on small or messy datasets is suspicious.\n\n2. **Performance drops significantly on new data**: Model would be overfitting because it \"cheated\".\n\n3. **Feature importance surprises**: Features that did not add much are suddenly highly predcitive.\n\n---\n\n## Preventing Data Leakage\n\nHere are some best practices to prevent data leakage:\n\n- **Proper Train/Test Splits**: Split your dataset before any preprocessing.\n\n- **Use Pipelines**: Scikit-learn pipelines ensure transformations only fit the training set.\n\n- **Feature Review**: Examine each feature for potential post event or future information.\n\n- **Time Aware Splitting**: For time-series data, always train on past data and test on future data.\n\n- **Cross-Validation Care**: Make sure folds do not leak information across training/testing partitions.\n\n---\n\n## Why Data Leakage Matters in the Real World\n\nData leakage is not just a theoretical problem, it can have real consequences:\n\n- **Business**: Inflated model performance can lead to poor decisions or wasted budget.\n\n- **Healthcare**: Misleading predictions in patient risk could have serious outcomes.\n\n- **Finance**: Models predicting fraud or defaults could fail, causing losses.\n\n- **Reputation**: Publishing flawed models could damage credibility.\n\n---\n\n## Conclusion\n\nData leakage is often subtle but highly damaging. It can enter models through incorrect splits, post event features, or improper preprocessing. By following careful best practices including splitting data correctly, reviewing features, and using pipelines, you can build models that properly generalize and can be trusted. \n\n---\n\n## References\n\n\n","srcMarkdownNoYaml":"\n\n## Introduction\n\nHave you ever trained a machine learning model that performed perfectly in testing, then failed in the real world? Perhaps your model scored 98% accuracy during testing, but when deployed on new data, performance dropped drastically. This frustrating scenario often happens because of a subtle, yet serious mistake: **data leakage**.\n\nData leakage is one of the most common pitfalls in data science. It can make models appear more powerful than they actually are, leading to false confidence, wasted resources, or even costly mistakes when deployed. In this post, I will explain what data leakage is, explore the types and causes, provide examples, and show how to prevent it. By the end, you will have a clear understanding of how to keep your models reliable and trustworthy.\n\n---\n\n## What is Data Leakage?\n\nIn machine learning, **data leakage occurs when your model has access to information during training that it would not realistically have when making predictions in the real world**. In other words, your model \"cheats\" by learning patterns that are impossible to know ahead of time. This may sound simple, but it can happen in subtle ways, such as including future information in your features or accidentally mixing training and testing data. The result is an inflated performance metric during development, which can be disastrous when the model is deployed.\n\n**Key takeaway:** Data leakage makes your model look great on paper, but it does not generalize to real world data.\n\n---\n\n## Common Types of Data Leakage\n\nThese are some of the main ways data leakage can happen:\n\n### 1. Train-Test Contamination\n\nThis occurs when the same data ends up in both the training and testing sets. For instance, imagine shuffling a dataset incorrectly or scaling features before splitting. Your model \"sees\" test data during training, leading to artifically high evaluation scores. [@ibm:data-leakage]\n\nExample: Fitting transformations on the entire dataset, including test data. \n\n### 2. Target Leakage\n\nTarget leakage occurs when a feature includes information that would only be available after the target outcome happens. This type of leakage is especially common in datasets with timestamps or events. [@ibm:data-leakage]\n\nExample: Predicting hospital readmissions using a feature \"Number of Medications After Discharge\" would be target leakage because this information is only known after the patient leaves the hospital. \n\n### 3. Temporal/Time Based Leakage\n\nTime-series and sequential data are prone to leakage if future information is used to predict the past. [@wiki:leakage_ml]\n\nExample: Predicting stock prices using feeatures that include future market movements. Even if it seems like a valid feature, the model is essentially \"peaking into the future.\"\n\n### 4. Preprocessing Leakage\n\nSome preprocessing steps can inadvertently leak information if applied before the train-test split. [@sailpoint:data-leakage]\n\nExample: Feature selection using the whole dataset.\n\n---\n\n## Detecting Data Leakage\n\nHow do you know if your model is suffering from data leakage? Below are some indicators:\n\n1. **Unrealistically high performance**: 99% accuracy on small or messy datasets is suspicious.\n\n2. **Performance drops significantly on new data**: Model would be overfitting because it \"cheated\".\n\n3. **Feature importance surprises**: Features that did not add much are suddenly highly predcitive.\n\n---\n\n## Preventing Data Leakage\n\nHere are some best practices to prevent data leakage:\n\n- **Proper Train/Test Splits**: Split your dataset before any preprocessing.\n\n- **Use Pipelines**: Scikit-learn pipelines ensure transformations only fit the training set.\n\n- **Feature Review**: Examine each feature for potential post event or future information.\n\n- **Time Aware Splitting**: For time-series data, always train on past data and test on future data.\n\n- **Cross-Validation Care**: Make sure folds do not leak information across training/testing partitions.\n\n---\n\n## Why Data Leakage Matters in the Real World\n\nData leakage is not just a theoretical problem, it can have real consequences:\n\n- **Business**: Inflated model performance can lead to poor decisions or wasted budget.\n\n- **Healthcare**: Misleading predictions in patient risk could have serious outcomes.\n\n- **Finance**: Models predicting fraud or defaults could fail, causing losses.\n\n- **Reputation**: Publishing flawed models could damage credibility.\n\n---\n\n## Conclusion\n\nData leakage is often subtle but highly damaging. It can enter models through incorrect splits, post event features, or improper preprocessing. By following careful best practices including splitting data correctly, reviewing features, and using pipelines, you can build models that properly generalize and can be trusted. \n\n---\n\n## References\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"data-leakage.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.21","theme":["cosmo","brand"],"title":"Why Your Model Performs Too Well: Understanding Data Leakage","author":"Ashifa Hassam","date":"2026-01-16","categories":["machine learning","data science"],"bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}